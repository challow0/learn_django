{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(446)\n",
    "np.random.seed(446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and relation to numpy\n",
    "By this point, we have worked with numpy quite a bit. PyTorch's basic building block, the `tensor` is similar to numpy's `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_torch\n",
      "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n",
      "\n",
      "to and from numpy and torch\n",
      "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
      "\n",
      "x+y\n",
      "[0.1 0.2 0.3] [3. 4. 5.] tensor([3.1000, 4.2000, 5.3000])\n",
      "\n",
      "norm\n",
      "0.37416573867739417 tensor(0.3742)\n",
      "\n",
      "mean along the 0th dimension\n",
      "[2. 3.] tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# we create tensors in a similar way to numpy nd arrays\n",
    "x_numpy = np.array([0.1,  0.2, 0.3])\n",
    "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
    "print('x_numpy, x_torch')\n",
    "print(x_numpy, x_torch)\n",
    "print()\n",
    "\n",
    "# to and from numpy ,torch\n",
    "print('to and from numpy and torch')\n",
    "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
    "print()\n",
    "\n",
    "# we can do basic operations like +-*/\n",
    "y_numpy = np.array([3,4,5.])\n",
    "y_torch = torch.tensor([3,4,5.])\n",
    "print('x+y')\n",
    "print(x_numpy, y_numpy, x_torch + y_torch)\n",
    "print()\n",
    "\n",
    "# many funcitons that are in numpy are also in pytorch\n",
    "print('norm')\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
    "print()\n",
    "\n",
    "#to apply an operation along a dimision\n",
    "# we use the dim keyword argument instead of axis\n",
    "print('mean along the 0th dimension')\n",
    "x_numpy = np.array([[1,2],[3,4.]])\n",
    "x_torch = torch.tensor([[1,2],[3,4.]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor.view()\n",
    "We can use the `Tensor.view()` function to reshape tensors similarly to `numpy.reshape()`\n",
    "\n",
    "It can also automatically calculate the correct dimension if a `-1` is passed in. This is useful if we are working with batches, but the batch size is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 28, 28])\n",
      "torch.Size([10000, 3, 784])\n",
      "torch.Size([10000, 3, 784])\n"
     ]
    }
   ],
   "source": [
    "# \"MINST\"\n",
    "N, C, W, H = 10000, 3, 28, 28\n",
    "X = torch.randn((N, C, W, H))\n",
    "\n",
    "print(X.shape)\n",
    "print(X.view(N, C, 784).shape)\n",
    "print(X.view(-1, C, 784).shape) # automatically choose the 0th dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BROADCASTING SEMANTICS`\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "Each tensor has at least one dimension.\n",
    "\n",
    "When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch operations support NumPy Broadcasting Semantics.\n",
    "x=torch.empty(5,1,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "print((x+y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graphs\n",
    "\n",
    "What's special about PyTorch's `tensor` object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.\n",
    "\n",
    "Consider the expression $e=(a+b)*(b+1)$ with values $a=2, b=1$. We can draw the evaluated computation graph as\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In PyTorch, we can write this as\n",
    "![tree-img](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png)\n",
    "\n",
    "[source](https://colah.github.io/posts/2015-08-Backprop/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(3., grad_fn=<AddBackward0>)\n",
      "d tensor(2., grad_fn=<AddBackward0>)\n",
      "e tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True) # we set requires_grad = True to let Pytorch know to keep the graph\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d\n",
    "print('c', c)\n",
    "print('d', d)\n",
    "print('e', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SEMANTICS\n",
    "It's easy cupy tensor from cpu to gpu or from gpu to cpu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4480, 0.9784, 0.7896, 0.0058, 0.4558, 0.8809, 0.2683, 0.7543, 0.1050,\n",
      "        0.3880])\n",
      "tensor([0.4480, 0.9784, 0.7896, 0.0058, 0.4558, 0.8809, 0.2683, 0.7543, 0.1050,\n",
      "        0.3880])\n",
      "tensor([0.4480, 0.9784, 0.7896, 0.0058, 0.4558, 0.8809, 0.2683, 0.7543, 0.1050,\n",
      "        0.3880], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x = x.to(cpu)\n",
    "print(x)\n",
    "x = x.to(gpu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as an auto grad framework\n",
    "\n",
    "Now that we have seen that PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
    "\n",
    "Consider the function $f(x) = (x-2)^2$.\n",
    "\n",
    "Q: Compute $\\frac{d}{dx} f(x)$ and then compute $f'(1)$.\n",
    "\n",
    "We make a `backward()` call on the leaf variable (`y`) in the computation, computing all the gradients of `y` at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x - 2) ** 2\n",
    "def fp(x):\n",
    "    return 2*(x - 2)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))\n",
    "print('PyTorch\\'s f\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also find gradients of functions.\n",
    "\n",
    "Let $w = [w_1, w_2]^T$\n",
    "\n",
    "Consider $g(w) = 2w_1w_2 + w_2\\cos(w_1)$\n",
    "\n",
    "Q: Compute $\\nabla_w g(w)$ and verify $\\nabla_w g([\\pi,1]) = [2, \\pi - 1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical grad g(w) tensor([2.0000, 5.2832])\n",
      "PyTorch's grad g(w) tensor([2.0000, 5.2832])\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
    "\n",
    "def grad_g(w):\n",
    "    return torch.tensor([2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0])])\n",
    "\n",
    "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
    "\n",
    "z = g(w)\n",
    "z.backward()\n",
    "\n",
    "print('Analytical grad g(w)', grad_g(w))\n",
    "print('PyTorch\\'s grad g(w)', w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the gradients\n",
    "Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!\n",
    "\n",
    "Let $f$ the same function we defined above.\n",
    "\n",
    "Q: What is the value of $x$ that minimizes $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tx,\tf(x),\tf'(x),\tf'(x) pytorch\n",
      "0,\t5.000,\t9.000,\t6.000,\t6.000\n",
      "1,\t3.500,\t2.250,\t3.000,\t3.000\n",
      "2,\t2.750,\t0.562,\t1.500,\t1.500\n",
      "3,\t2.375,\t0.141,\t0.750,\t0.750\n",
      "4,\t2.188,\t0.035,\t0.375,\t0.375\n",
      "5,\t2.094,\t0.009,\t0.188,\t0.188\n",
      "6,\t2.047,\t0.002,\t0.094,\t0.094\n",
      "7,\t2.023,\t0.001,\t0.047,\t0.047\n",
      "8,\t2.012,\t0.000,\t0.023,\t0.023\n",
      "9,\t2.006,\t0.000,\t0.012,\t0.012\n",
      "10,\t2.003,\t0.000,\t0.006,\t0.006\n",
      "11,\t2.001,\t0.000,\t0.003,\t0.003\n",
      "12,\t2.001,\t0.000,\t0.001,\t0.001\n",
      "13,\t2.000,\t0.000,\t0.001,\t0.001\n",
      "14,\t2.000,\t0.000,\t0.000,\t0.000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "\n",
    "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() # compute the gradient\n",
    "    \n",
    "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "    \n",
    "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
    "    \n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Now, instead of minimizing a made-up function, lets minimize a loss function on some made-up data.\n",
    "\n",
    "We will implement Gradient Descent in order to solve the task of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape torch.Size([50, 2])\n",
      "y shape torch.Size([50, 1])\n",
      "w shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# make a simple linear dataset with some noise\n",
    "\n",
    "d = 2\n",
    "n = 50\n",
    "X = torch.randn(n, d)\n",
    "true_w = torch.tensor([[-1.0],[2.0]])\n",
    "y = X @ true_w + torch.randn(n, 1) * 0.1\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# print(true_w)\n",
    "\n",
    "# print(y)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "print('w shape', true_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: dimensions\n",
    "PyTorch does a lot of operations on batches of data. The convention is to have your data be of size $(N, d)$ where $N$ is the size of the batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "To verify PyTorch is computing the gradients correctly, let's recall the gradient for the RSS objective:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L}_{RSS}(w; X) = \\nabla_w\\frac{1}{n} ||y - Xw||_2^2 = -\\frac{2}{n}X^T(y-Xw)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient [ 5.699138 -4.140066]\n",
      "PyTorch's gradient [ 5.6991386 -4.1400657]\n"
     ]
    }
   ],
   "source": [
    "# define a linear model with no bias\n",
    "def model(X, w):\n",
    "    return X @ w\n",
    "\n",
    "# the residual sum of squares loss function\n",
    "def rss(y, y_hat):\n",
    "    return torch.norm(y - y_hat)**2 / n\n",
    "\n",
    "# analytical expression for the gradient\n",
    "def grad_rss(X, y, w):\n",
    "    return -2*X.t() @ (y - X @ w) / n\n",
    "\n",
    "w = torch.tensor([[1.],[0]], requires_grad=True)\n",
    "y_hat = model(X, w)\n",
    "\n",
    "loss = rss(y, y_hat)\n",
    "loss.backward()\n",
    "\n",
    "print('Analytical gradient', grad_rss(X, y, w).detach().view(2).numpy())\n",
    "print('PyTorch\\'s gradient', w.grad.view(2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen PyTorch is doing the right think, let's use the gradients!\n",
    "\n",
    "## Linear regression using GD with automatically computed derivatives\n",
    "\n",
    "We will now use the gradients to run the gradient descent algorithm.\n",
    "\n",
    "Note: This example is an illustration to connect ideas we have seen before to PyTorch's way of doing things. We will see how to do this in the \"PyTorchic\" way in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tloss,\tw\n",
      "0,\t9.92,\t[-0.13982773  0.8280131 ]\n",
      "1,\t2.54,\t[-0.38633484  1.0729955 ]\n",
      "2,\t1.48,\t[-0.5631086  1.2676816]\n",
      "3,\t0.87,\t[-0.68993586  1.4223369 ]\n",
      "4,\t0.52,\t[-0.78097606  1.5451494 ]\n",
      "5,\t0.31,\t[-0.84636474  1.6426448 ]\n",
      "6,\t0.19,\t[-0.89335895  1.7200202 ]\n",
      "7,\t0.12,\t[-0.9271563  1.7814121]\n",
      "8,\t0.08,\t[-0.951481   1.8301114]\n",
      "9,\t0.05,\t[-0.96900225  1.8687341 ]\n",
      "10,\t0.03,\t[-0.9816342  1.8993598]\n",
      "11,\t0.02,\t[-0.99075    1.9236403]\n",
      "12,\t0.02,\t[-0.9973353  1.9428873]\n",
      "13,\t0.01,\t[-1.002098   1.9581423]\n",
      "14,\t0.01,\t[-1.0055467  1.9702318]\n",
      "15,\t0.01,\t[-1.0080472  1.9798117]\n",
      "16,\t0.01,\t[-1.0098629  1.9874021]\n",
      "17,\t0.01,\t[-1.0111833  1.9934157]\n",
      "18,\t0.01,\t[-1.012145   1.9981797]\n",
      "19,\t0.01,\t[-1.0128468  2.0019534]\n",
      "\n",
      "true w\t\t [-1.  2.]\n",
      "estimated w\t [-1.0128468  2.0019534]\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "\n",
    "print('iter,\\tloss,\\tw')\n",
    "for i in range (20):\n",
    "    y_hat = model(X, w)\n",
    "    loss = rss(y, y_hat)\n",
    "    \n",
    "    loss.backward() # compute the gradient of the loss\n",
    "    \n",
    "    w.data = w.data - step_size * w.grad # do a gradient descent step\n",
    "    \n",
    "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), w.view(2).detach().numpy()))\n",
    "\n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting\n",
    "    # The detach_() is for efficiency. \n",
    "    w.grad.detach()\n",
    "    w.grad.zero_()\n",
    "    \n",
    "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
    "print('estimated w\\t', w.view(2).detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
